{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadik-tofik/NLP_Assignment/blob/main/NLP_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBVxecqg5QjB"
      },
      "source": [
        "# Natural Language Processing (NLP) Assignment\n",
        "This assignment will guide you through the basic concepts of Natural Language Processing including:\n",
        "- Text preprocessing\n",
        "- Tokenization and N-grams\n",
        "- Named Entity Recognition (NER)\n",
        "- Converting text into numbers (vectorization)\n",
        "- Word embeddings (for experienced learners)\n",
        "\n",
        "You can run and modify the code cells below to complete the tasks."
      ],
      "id": "jBVxecqg5QjB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrWBp-QT5QjI",
        "outputId": "047eb4fd-c36a-4329-c533-4baffb2c23a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "id": "lrWBp-QT5QjI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le_AAvLT5QjM"
      },
      "source": [
        "## 1. Text Preprocessing\n",
        "Clean the following text by converting it to lowercase, removing punctuation and stop words."
      ],
      "id": "Le_AAvLT5QjM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aLbAsLc5QjN",
        "outputId": "44f59063-9cf2-401c-da59-ef96a2b3fd99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', 'text', 'preprocessing']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove punctuation and stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Define the text to preprocess\n",
        "text = \"This is an example sentence for text preprocessing.\" # You can replace this with the text you want to clean\n",
        "\n",
        "# Print cleaned tokens\n",
        "cleaned_tokens = preprocess(text)\n",
        "print(cleaned_tokens)"
      ],
      "id": "1aLbAsLc5QjN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0d4_Pa35QjO"
      },
      "source": [
        "## 2. Tokenization and N-grams\n",
        "Generate bigrams (2-grams) from the cleaned tokens."
      ],
      "id": "G0d4_Pa35QjO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-FCTKzU5QjP",
        "outputId": "9b893f90-dfe7-44ef-9936-9e2644017416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams: [('example', 'sentence'), ('sentence', 'text'), ('text', 'preprocessing')]\n"
          ]
        }
      ],
      "source": [
        "# Generate bigrams from cleaned tokens\n",
        "bigrams = list(ngrams(cleaned_tokens, 2))\n",
        "print(\"Bigrams:\", bigrams)"
      ],
      "id": "D-FCTKzU5QjP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_TiYkSi5QjQ"
      },
      "source": [
        "## 3. Named Entity Recognition (NER)\n",
        "Use spaCy to perform NER on a new sentence."
      ],
      "id": "3_TiYkSi5QjQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x52hvisb5QjR",
        "outputId": "ac12cfdc-69e2-41ea-b640-aa98c4ca3014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barack Obama PERSON\n",
            "Hawaii GPE\n",
            "2008 DATE\n"
          ]
        }
      ],
      "source": [
        "# Example sentence\n",
        "sentence = \"Barack Obama was born in Hawaii and was elected president in 2008.\"\n",
        "doc = nlp(sentence)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "id": "x52hvisb5QjR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_2elhhG5QjS"
      },
      "source": [
        "## 4. Converting Text to Numbers\n",
        "Use CountVectorizer and TfidfVectorizer to convert a list of sentences into numeric vectors."
      ],
      "id": "o_2elhhG5QjS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcZ7-XwQ5QjT",
        "outputId": "16b60e48-cda5-41a5-8ce7-7bc8e1906b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Vectorizer Output:\n",
            " [[0 0 0 0 1 1 1 0 0 0 0 0]\n",
            " [1 0 1 1 0 0 0 1 1 1 1 0]\n",
            " [1 1 1 0 0 0 0 0 0 0 0 1]]\n",
            "\n",
            "TF-IDF Vectorizer Output:\n",
            " [[0.         0.         0.         0.         0.57735027 0.57735027\n",
            "  0.57735027 0.         0.         0.         0.         0.        ]\n",
            " [0.30650422 0.         0.30650422 0.40301621 0.         0.\n",
            "  0.         0.40301621 0.40301621 0.40301621 0.40301621 0.        ]\n",
            " [0.42804604 0.5628291  0.42804604 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.5628291 ]]\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"I love machine learning.\",\n",
        "    \"Natural language processing is a part of AI.\",\n",
        "    \"AI is the future.\"\n",
        "]\n",
        "\n",
        "# CountVectorizer\n",
        "count_vec = CountVectorizer()\n",
        "X_count = count_vec.fit_transform(sentences)\n",
        "print(\"Count Vectorizer Output:\\n\", X_count.toarray())\n",
        "\n",
        "# TfidfVectorizer\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vec.fit_transform(sentences)\n",
        "print(\"\\nTF-IDF Vectorizer Output:\\n\", X_tfidf.toarray())"
      ],
      "id": "mcZ7-XwQ5QjT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPb36Tgv5QjV"
      },
      "source": [
        "## 5. Word Embeddings (Advanced)\n",
        "Use spaCy to get word vectors (embeddings) for given words."
      ],
      "id": "BPb36Tgv5QjV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL5NMbLR5QjW",
        "outputId": "2aea2424-6a53-45ad-c21e-b17f86172618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'machine':\n",
            " [-0.7506131  -0.57648134  0.64351434  0.34771815  0.45008683 -0.31984502\n",
            "  1.3374304   0.6823808  -0.2497879   0.01502723  0.20069116 -0.53005815\n",
            " -0.32142693  0.6083895   0.5911227   1.3969526  -1.3394687  -0.49667895\n",
            "  0.93146133  0.76211375 -0.63203835  1.1820786  -0.8377956   0.02632815\n",
            " -0.2938518   0.6069318   1.5544686  -0.04658484 -0.4521918   0.48126173\n",
            "  0.02117339  0.9538507   0.38607836  0.03060612 -1.2614324  -0.71200204\n",
            " -0.0582068   0.9979756   0.39407492  0.03983042 -0.9099759  -0.30680424\n",
            "  0.81676024  0.40132928 -0.6591901  -0.52833277 -0.1020698  -0.39648485\n",
            " -0.2746659  -0.5868281   0.11670423 -0.02715784  0.10342616 -0.71523666\n",
            "  0.78196347  0.26182324  1.2007883   0.40819865 -0.81201667  0.10142356\n",
            " -0.92464274 -0.0610747  -0.28506368 -0.27212036 -0.06658131  0.21739644\n",
            " -0.34570417 -0.7191127  -0.6493219  -0.07578599 -0.26895642  0.25262332\n",
            "  0.8506174   0.5730195  -0.10925089 -0.48964912 -0.33062595 -0.6904906\n",
            "  0.58739054  0.52517235 -0.47667012 -0.37759903 -0.88804555 -0.58199006\n",
            "  0.22513609  1.8321126  -0.54407513  0.07091011 -1.0318854   0.15322405\n",
            " -0.44268894  0.00544617  0.5723448   0.2448978  -0.8239452   0.42069194]\n"
          ]
        }
      ],
      "source": [
        "# Note: en_core_web_sm does not have word vectors. You can install and use en_core_web_md\n",
        "# Uncomment below to install and load the medium model if needed.\n",
        "# !python -m spacy download en_core_web_md\n",
        "# nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Example word vector\n",
        "word = nlp(\"machine\")[0]\n",
        "print(\"Vector for 'machine':\\n\", word.vector)"
      ],
      "id": "iL5NMbLR5QjW"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}