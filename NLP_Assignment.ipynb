{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadik-tofik/NLP_Assignment/blob/main/NLP_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBVxecqg5QjB"
      },
      "source": [
        "# Natural Language Processing (NLP) Assignment\n",
        "This assignment will guide you through the basic concepts of Natural Language Processing including:\n",
        "- Text preprocessing\n",
        "- Tokenization and N-grams\n",
        "- Named Entity Recognition (NER)\n",
        "- Converting text into numbers (vectorization)\n",
        "- Word embeddings (for experienced learners)\n",
        "\n",
        "You can run and modify the code cells below to complete the tasks."
      ],
      "id": "jBVxecqg5QjB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrWBp-QT5QjI",
        "outputId": "047eb4fd-c36a-4329-c533-4baffb2c23a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "id": "lrWBp-QT5QjI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le_AAvLT5QjM"
      },
      "source": [
        "## 1. Text Preprocessing\n",
        "Clean the following text by converting it to lowercase, removing punctuation and stop words."
      ],
      "id": "Le_AAvLT5QjM"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aLbAsLc5QjN",
        "outputId": "476b2ddf-3ea5-4811-a9e1-0cd5eb1e5f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'nlp', 'fascinating', 'field', 'combines', 'computer', 'science', 'artificial', 'intelligence', 'linguistics', 'enable', 'computers', 'understand', 'process', 'human', 'language', 'wide', 'range', 'applications', 'including', 'machine', 'translation', 'sentiment', 'analysis', 'chatbots', 'speech', 'recognition', 'process', 'typically', 'involves', 'several', 'steps', 'starting', 'text', 'preprocessing', 'includes', 'tasks', 'like', 'lowercasing', 'tokenization', 'removing', 'punctuation', 'stop', 'words', 'following', 'preprocessing', 'techniques', 'like', 'stemming', 'lemmatization', 'applied', 'reduce', 'words', 'root', 'forms', 'tokenization', 'breaks', 'text', 'individual', 'words', 'sub-word', 'units', 'n-grams', 'capture', 'sequences', 'tokens', 'providing', 'context', 'named', 'entity', 'recognition', 'ner', 'another', 'important', 'task', 'nlp', 'identifies', 'classifies', 'named', 'entities', 'text', 'people', 'organizations', 'locations', 'converting', 'text', 'numerical', 'representations', 'crucial', 'machine', 'learning', 'models', 'methods', 'like', 'countvectorizer', 'tf-idf', 'commonly', 'used', 'purpose', 'advanced', 'techniques', 'involve', 'word', 'embeddings', 'represent', 'words', 'dense', 'vectors', 'continuous', 'vector', 'space', 'capturing', 'semantic', 'relationships', 'words']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove punctuation and stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Define the text to preprocess\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field that combines computer science, artificial intelligence, and linguistics to enable computers to understand and process human language. It has a wide range of applications, including machine translation, sentiment analysis, chatbots, and speech recognition. The process typically involves several steps, starting with text preprocessing, which includes tasks like lowercasing, tokenization, and removing punctuation and stop words. Following preprocessing, techniques like stemming and lemmatization can be applied to reduce words to their root forms. Tokenization breaks down text into individual words or sub-word units, while n-grams capture sequences of these tokens, providing more context. Named Entity Recognition (NER) is another important task in NLP, which identifies and classifies named entities in text, such as people, organizations, and locations. Converting text into numerical representations is crucial for machine learning models, and methods like CountVectorizer and TF-IDF are commonly used for this purpose. More advanced techniques involve word embeddings, which represent words as dense vectors in a continuous vector space, capturing semantic relationships between words.\"\n",
        "\n",
        "\n",
        "# Print cleaned tokens\n",
        "cleaned_tokens = preprocess(text)\n",
        "print(cleaned_tokens)"
      ],
      "id": "1aLbAsLc5QjN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0d4_Pa35QjO"
      },
      "source": [
        "## 2. Tokenization and N-grams\n",
        "Generate bigrams (2-grams) from the cleaned tokens."
      ],
      "id": "G0d4_Pa35QjO"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-FCTKzU5QjP",
        "outputId": "5a88eff7-04d9-4507-8182-bcf8223cf062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams: [('natural', 'language', 'processing'), ('language', 'processing', 'nlp'), ('processing', 'nlp', 'fascinating'), ('nlp', 'fascinating', 'field'), ('fascinating', 'field', 'combines'), ('field', 'combines', 'computer'), ('combines', 'computer', 'science'), ('computer', 'science', 'artificial'), ('science', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'linguistics'), ('intelligence', 'linguistics', 'enable'), ('linguistics', 'enable', 'computers'), ('enable', 'computers', 'understand'), ('computers', 'understand', 'process'), ('understand', 'process', 'human'), ('process', 'human', 'language'), ('human', 'language', 'wide'), ('language', 'wide', 'range'), ('wide', 'range', 'applications'), ('range', 'applications', 'including'), ('applications', 'including', 'machine'), ('including', 'machine', 'translation'), ('machine', 'translation', 'sentiment'), ('translation', 'sentiment', 'analysis'), ('sentiment', 'analysis', 'chatbots'), ('analysis', 'chatbots', 'speech'), ('chatbots', 'speech', 'recognition'), ('speech', 'recognition', 'process'), ('recognition', 'process', 'typically'), ('process', 'typically', 'involves'), ('typically', 'involves', 'several'), ('involves', 'several', 'steps'), ('several', 'steps', 'starting'), ('steps', 'starting', 'text'), ('starting', 'text', 'preprocessing'), ('text', 'preprocessing', 'includes'), ('preprocessing', 'includes', 'tasks'), ('includes', 'tasks', 'like'), ('tasks', 'like', 'lowercasing'), ('like', 'lowercasing', 'tokenization'), ('lowercasing', 'tokenization', 'removing'), ('tokenization', 'removing', 'punctuation'), ('removing', 'punctuation', 'stop'), ('punctuation', 'stop', 'words'), ('stop', 'words', 'following'), ('words', 'following', 'preprocessing'), ('following', 'preprocessing', 'techniques'), ('preprocessing', 'techniques', 'like'), ('techniques', 'like', 'stemming'), ('like', 'stemming', 'lemmatization'), ('stemming', 'lemmatization', 'applied'), ('lemmatization', 'applied', 'reduce'), ('applied', 'reduce', 'words'), ('reduce', 'words', 'root'), ('words', 'root', 'forms'), ('root', 'forms', 'tokenization'), ('forms', 'tokenization', 'breaks'), ('tokenization', 'breaks', 'text'), ('breaks', 'text', 'individual'), ('text', 'individual', 'words'), ('individual', 'words', 'sub-word'), ('words', 'sub-word', 'units'), ('sub-word', 'units', 'n-grams'), ('units', 'n-grams', 'capture'), ('n-grams', 'capture', 'sequences'), ('capture', 'sequences', 'tokens'), ('sequences', 'tokens', 'providing'), ('tokens', 'providing', 'context'), ('providing', 'context', 'named'), ('context', 'named', 'entity'), ('named', 'entity', 'recognition'), ('entity', 'recognition', 'ner'), ('recognition', 'ner', 'another'), ('ner', 'another', 'important'), ('another', 'important', 'task'), ('important', 'task', 'nlp'), ('task', 'nlp', 'identifies'), ('nlp', 'identifies', 'classifies'), ('identifies', 'classifies', 'named'), ('classifies', 'named', 'entities'), ('named', 'entities', 'text'), ('entities', 'text', 'people'), ('text', 'people', 'organizations'), ('people', 'organizations', 'locations'), ('organizations', 'locations', 'converting'), ('locations', 'converting', 'text'), ('converting', 'text', 'numerical'), ('text', 'numerical', 'representations'), ('numerical', 'representations', 'crucial'), ('representations', 'crucial', 'machine'), ('crucial', 'machine', 'learning'), ('machine', 'learning', 'models'), ('learning', 'models', 'methods'), ('models', 'methods', 'like'), ('methods', 'like', 'countvectorizer'), ('like', 'countvectorizer', 'tf-idf'), ('countvectorizer', 'tf-idf', 'commonly'), ('tf-idf', 'commonly', 'used'), ('commonly', 'used', 'purpose'), ('used', 'purpose', 'advanced'), ('purpose', 'advanced', 'techniques'), ('advanced', 'techniques', 'involve'), ('techniques', 'involve', 'word'), ('involve', 'word', 'embeddings'), ('word', 'embeddings', 'represent'), ('embeddings', 'represent', 'words'), ('represent', 'words', 'dense'), ('words', 'dense', 'vectors'), ('dense', 'vectors', 'continuous'), ('vectors', 'continuous', 'vector'), ('continuous', 'vector', 'space'), ('vector', 'space', 'capturing'), ('space', 'capturing', 'semantic'), ('capturing', 'semantic', 'relationships'), ('semantic', 'relationships', 'words')]\n"
          ]
        }
      ],
      "source": [
        "# Generate bigrams from cleaned tokens\n",
        "bigrams = list(ngrams(cleaned_tokens, 3))\n",
        "print(\"Bigrams:\", bigrams)"
      ],
      "id": "D-FCTKzU5QjP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_TiYkSi5QjQ"
      },
      "source": [
        "## 3. Named Entity Recognition (NER)\n",
        "Use spaCy to perform NER on a new sentence."
      ],
      "id": "3_TiYkSi5QjQ"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x52hvisb5QjR",
        "outputId": "b17b5206-e3f9-4515-e001-b2c5ca588c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "Mountain View GPE\n",
            "California GPE\n",
            "Larry Page PERSON\n",
            "Sergey Brin PERSON\n",
            "1998 DATE\n",
            "Sundar Pichai PERSON\n"
          ]
        }
      ],
      "source": [
        "# Example sentence\n",
        "sentence = \"The Google headquarters in Mountain View, California, was founded by Larry Page and Sergey Brin in 1998. Sundar Pichai is the current CEO.\"\n",
        "doc = nlp(sentence)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "id": "x52hvisb5QjR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_2elhhG5QjS"
      },
      "source": [
        "## 4. Converting Text to Numbers\n",
        "Use CountVectorizer and TfidfVectorizer to convert a list of sentences into numeric vectors."
      ],
      "id": "o_2elhhG5QjS"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcZ7-XwQ5QjT",
        "outputId": "ceb4cf33-22e9-425a-aaed-9c118efe80fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Vectorizer Output:\n",
            " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1]]\n",
            "\n",
            "TF-IDF Vectorizer Output:\n",
            " [[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.53828134 0.64846464 0.53828134\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.34594225 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.25672902 0.         0.34594225 0.         0.         0.\n",
            "  0.41675476 0.         0.41675476 0.41675476 0.41675476 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.47386908 0.         0.         0.         0.         0.\n",
            "  0.57086754 0.         0.         0.         0.         0.\n",
            "  0.35166548 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.57086754 0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.51587769 0.51587769 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.31779067 0.         0.         0.42822279 0.         0.42822279\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.41925708 0.         0.         0.\n",
            "  0.         0.         0.41925708 0.41925708 0.         0.\n",
            "  0.         0.         0.34801938 0.         0.         0.\n",
            "  0.         0.41925708 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.41925708 0.\n",
            "  0.        ]\n",
            " [0.         0.43115145 0.         0.         0.         0.43115145\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.26559766 0.43115145 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.43115145 0.         0.         0.         0.         0.43115145\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.37796447 0.         0.         0.37796447 0.37796447\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.37796447\n",
            "  0.         0.37796447 0.         0.37796447 0.         0.\n",
            "  0.37796447]]\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"I love machine learning.\",\n",
        "    \"Natural language processing is a part of AI.\",\n",
        "    \"AI is the future.\",\n",
        "    \"Machine learning is a fascinating field.\",\n",
        "    \"NLP helps computers understand human language.\",\n",
        "    \"Vectorization is key for text analysis.\",\n",
        "    \"TF-IDF gives importance to relevant words.\"\n",
        "]\n",
        "\n",
        "# CountVectorizer\n",
        "count_vec = CountVectorizer()\n",
        "X_count = count_vec.fit_transform(sentences)\n",
        "print(\"Count Vectorizer Output:\\n\", X_count.toarray())\n",
        "\n",
        "# TfidfVectorizer\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vec.fit_transform(sentences)\n",
        "print(\"\\nTF-IDF Vectorizer Output:\\n\", X_tfidf.toarray())"
      ],
      "id": "mcZ7-XwQ5QjT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPb36Tgv5QjV"
      },
      "source": [
        "## 5. Word Embeddings (Advanced)\n",
        "Use spaCy to get word vectors (embeddings) for given words."
      ],
      "id": "BPb36Tgv5QjV"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL5NMbLR5QjW",
        "outputId": "027e1885-fff2-4093-edf3-83dbcfe239fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'machine':\n",
            " [-0.7506131  -0.57648134  0.64351434  0.34771815  0.45008683 -0.31984502\n",
            "  1.3374304   0.6823808  -0.2497879   0.01502723  0.20069116 -0.53005815\n",
            " -0.32142693  0.6083895   0.5911227   1.3969526  -1.3394687  -0.49667895\n",
            "  0.93146133  0.76211375 -0.63203835  1.1820786  -0.8377956   0.02632815\n",
            " -0.2938518   0.6069318   1.5544686  -0.04658484 -0.4521918   0.48126173\n",
            "  0.02117339  0.9538507   0.38607836  0.03060612 -1.2614324  -0.71200204\n",
            " -0.0582068   0.9979756   0.39407492  0.03983042 -0.9099759  -0.30680424\n",
            "  0.81676024  0.40132928 -0.6591901  -0.52833277 -0.1020698  -0.39648485\n",
            " -0.2746659  -0.5868281   0.11670423 -0.02715784  0.10342616 -0.71523666\n",
            "  0.78196347  0.26182324  1.2007883   0.40819865 -0.81201667  0.10142356\n",
            " -0.92464274 -0.0610747  -0.28506368 -0.27212036 -0.06658131  0.21739644\n",
            " -0.34570417 -0.7191127  -0.6493219  -0.07578599 -0.26895642  0.25262332\n",
            "  0.8506174   0.5730195  -0.10925089 -0.48964912 -0.33062595 -0.6904906\n",
            "  0.58739054  0.52517235 -0.47667012 -0.37759903 -0.88804555 -0.58199006\n",
            "  0.22513609  1.8321126  -0.54407513  0.07091011 -1.0318854   0.15322405\n",
            " -0.44268894  0.00544617  0.5723448   0.2448978  -0.8239452   0.42069194]\n",
            "Vector for 'language':\n",
            " [-1.0077751  -0.5772448   0.807741    0.5020691  -0.16562825  0.07199004\n",
            "  1.5911919   0.07365999 -0.17684056 -0.02976018  0.04064542 -0.8063699\n",
            " -0.5678126   1.1565169   0.6441366   0.58917475 -0.8043104  -0.569604\n",
            " -0.35359877 -0.6104816  -0.03494999  1.8956724  -0.63763875 -0.04395077\n",
            "  0.37297648  0.732082    0.8012695   0.11868745 -0.2065643   0.87622094\n",
            "  0.4967224   1.2606642  -0.02712551  0.04085238 -1.016993   -0.4143535\n",
            " -0.4092837  -0.01205276  0.44511455  0.6789813  -0.06373318  0.22727537\n",
            " -0.7056403   1.2506526   0.30618948 -0.12826627 -1.4574554  -1.1673295\n",
            "  0.01331332 -0.50046426  0.01249845  0.21698806 -0.38005668 -0.48934802\n",
            "  1.2839065   0.02326697  0.69082963 -0.0270623   0.5188805  -0.04083882\n",
            " -1.3873837   0.20032963 -0.3816198  -0.48977032 -0.41097274 -0.20697615\n",
            "  0.2127003  -1.240556   -0.50670636 -0.35174328 -0.292675    0.21359915\n",
            "  1.2180498   0.02751341  0.3376719  -0.70374835  0.29279193 -0.8862354\n",
            " -0.21981026 -0.02282158 -0.5171051  -0.44282168 -0.23441453 -0.11484504\n",
            "  1.2318505   0.7666715   0.19958344  0.06805015 -0.7548471   0.2550904\n",
            " -1.5327996   0.36906904  0.75184417  0.2672174   0.09330302  0.4115603 ]\n"
          ]
        }
      ],
      "source": [
        "# Note: en_core_web_sm does not have word vectors. You can install and use en_core_web_md\n",
        "# Uncomment below to install and load the medium model if needed.\n",
        "# !python -m spacy download en_core_web_md\n",
        "# nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Example word vector\n",
        "word = nlp(\"machine\")[0]\n",
        "print(\"Vector for 'machine':\\n\", word.vector)\n",
        "# Another example word vector\n",
        "word_language = nlp(\"language\")[0]\n",
        "print(\"Vector for 'language':\\n\", word_language.vector)"
      ],
      "id": "iL5NMbLR5QjW"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}